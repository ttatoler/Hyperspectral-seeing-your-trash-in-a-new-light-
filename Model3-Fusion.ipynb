{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea472f5-19d4-4b4a-9dce-4ae8e7f496de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 26/26 [00:55<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.1565, Train Mean IoU: 0.0684\n",
      "  film: 0.1131\n",
      "  basket: 0.0969\n",
      "  cardboard: 0.0217\n",
      "  video_tape: 0.0174\n",
      "  filament: 0.0378\n",
      "  bag: 0.1234\n",
      "Epoch [1/50], Val Mean IoU: 0.0713\n",
      "  film: 0.1234\n",
      "  basket: 0.1019\n",
      "  cardboard: 0.0094\n",
      "  video_tape: 0.0145\n",
      "  filament: 0.0164\n",
      "  bag: 0.1624\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 26/26 [01:01<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Train Loss: 1.0311, Train Mean IoU: 0.0946\n",
      "  film: 0.1634\n",
      "  basket: 0.1181\n",
      "  cardboard: 0.0533\n",
      "  video_tape: 0.0290\n",
      "  filament: 0.0102\n",
      "  bag: 0.1935\n",
      "Epoch [2/50], Val Mean IoU: 0.1063\n",
      "  film: 0.1776\n",
      "  basket: 0.1388\n",
      "  cardboard: 0.0581\n",
      "  video_tape: 0.0225\n",
      "  filament: 0.0152\n",
      "  bag: 0.2255\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 26/26 [00:53<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Train Loss: 0.9319, Train Mean IoU: 0.1199\n",
      "  film: 0.1576\n",
      "  basket: 0.0630\n",
      "  cardboard: 0.2788\n",
      "  video_tape: 0.0069\n",
      "  filament: 0.0138\n",
      "  bag: 0.1994\n",
      "Epoch [3/50], Val Mean IoU: 0.1166\n",
      "  film: 0.1778\n",
      "  basket: 0.0796\n",
      "  cardboard: 0.1878\n",
      "  video_tape: 0.0089\n",
      "  filament: 0.0170\n",
      "  bag: 0.2284\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 26/26 [00:55<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Train Loss: 0.8423, Train Mean IoU: 0.1581\n",
      "  film: 0.2169\n",
      "  basket: 0.3241\n",
      "  cardboard: 0.0326\n",
      "  video_tape: 0.0901\n",
      "  filament: 0.0906\n",
      "  bag: 0.1945\n",
      "Epoch [4/50], Val Mean IoU: 0.1627\n",
      "  film: 0.2288\n",
      "  basket: 0.3448\n",
      "  cardboard: 0.0381\n",
      "  video_tape: 0.0869\n",
      "  filament: 0.0591\n",
      "  bag: 0.2184\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 26/26 [00:55<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Train Loss: 0.7397, Train Mean IoU: 0.2193\n",
      "  film: 0.3456\n",
      "  basket: 0.5174\n",
      "  cardboard: 0.0642\n",
      "  video_tape: 0.1081\n",
      "  filament: 0.0310\n",
      "  bag: 0.2497\n",
      "Epoch [5/50], Val Mean IoU: 0.2104\n",
      "  film: 0.3280\n",
      "  basket: 0.5071\n",
      "  cardboard: 0.0659\n",
      "  video_tape: 0.0919\n",
      "  filament: 0.0258\n",
      "  bag: 0.2438\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 26/26 [00:53<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Train Loss: 0.6783, Train Mean IoU: 0.2431\n",
      "  film: 0.5488\n",
      "  basket: 0.4347\n",
      "  cardboard: 0.0918\n",
      "  video_tape: 0.1140\n",
      "  filament: 0.0387\n",
      "  bag: 0.2305\n",
      "Epoch [6/50], Val Mean IoU: 0.2204\n",
      "  film: 0.4305\n",
      "  basket: 0.4347\n",
      "  cardboard: 0.1036\n",
      "  video_tape: 0.0999\n",
      "  filament: 0.0297\n",
      "  bag: 0.2241\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 26/26 [00:54<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Train Loss: 0.6361, Train Mean IoU: 0.2870\n",
      "  film: 0.4601\n",
      "  basket: 0.4370\n",
      "  cardboard: 0.3984\n",
      "  video_tape: 0.1152\n",
      "  filament: 0.0783\n",
      "  bag: 0.2331\n",
      "Epoch [7/50], Val Mean IoU: 0.2442\n",
      "  film: 0.3786\n",
      "  basket: 0.4536\n",
      "  cardboard: 0.2396\n",
      "  video_tape: 0.0982\n",
      "  filament: 0.0709\n",
      "  bag: 0.2241\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 26/26 [00:54<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Train Loss: 0.6206, Train Mean IoU: 0.2500\n",
      "  film: 0.4474\n",
      "  basket: 0.3938\n",
      "  cardboard: 0.2080\n",
      "  video_tape: 0.1112\n",
      "  filament: 0.0820\n",
      "  bag: 0.2574\n",
      "Epoch [8/50], Val Mean IoU: 0.2281\n",
      "  film: 0.3815\n",
      "  basket: 0.3965\n",
      "  cardboard: 0.1891\n",
      "  video_tape: 0.0936\n",
      "  filament: 0.0603\n",
      "  bag: 0.2475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 26/26 [00:53<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Train Loss: 0.5983, Train Mean IoU: 0.2482\n",
      "  film: 0.4707\n",
      "  basket: 0.4767\n",
      "  cardboard: 0.1004\n",
      "  video_tape: 0.1241\n",
      "  filament: 0.0552\n",
      "  bag: 0.2623\n",
      "Epoch [9/50], Val Mean IoU: 0.2302\n",
      "  film: 0.3955\n",
      "  basket: 0.4729\n",
      "  cardboard: 0.1139\n",
      "  video_tape: 0.1029\n",
      "  filament: 0.0476\n",
      "  bag: 0.2486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 26/26 [00:53<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Train Loss: 0.5656, Train Mean IoU: 0.3156\n",
      "  film: 0.6086\n",
      "  basket: 0.5493\n",
      "  cardboard: 0.3328\n",
      "  video_tape: 0.0928\n",
      "  filament: 0.0593\n",
      "  bag: 0.2509\n",
      "Epoch [10/50], Val Mean IoU: 0.2645\n",
      "  film: 0.4487\n",
      "  basket: 0.5616\n",
      "  cardboard: 0.2166\n",
      "  video_tape: 0.0778\n",
      "  filament: 0.0491\n",
      "  bag: 0.2330\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 26/26 [00:54<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Train Loss: 0.5386, Train Mean IoU: 0.3025\n",
      "  film: 0.4526\n",
      "  basket: 0.5466\n",
      "  cardboard: 0.3399\n",
      "  video_tape: 0.0872\n",
      "  filament: 0.0508\n",
      "  bag: 0.3381\n",
      "Epoch [11/50], Val Mean IoU: 0.2624\n",
      "  film: 0.3894\n",
      "  basket: 0.5432\n",
      "  cardboard: 0.2121\n",
      "  video_tape: 0.0751\n",
      "  filament: 0.0507\n",
      "  bag: 0.3040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 26/26 [00:54<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Train Loss: 0.5178, Train Mean IoU: 0.3498\n",
      "  film: 0.6588\n",
      "  basket: 0.5918\n",
      "  cardboard: 0.2325\n",
      "  video_tape: 0.1795\n",
      "  filament: 0.0399\n",
      "  bag: 0.3961\n",
      "Epoch [12/50], Val Mean IoU: 0.2990\n",
      "  film: 0.4931\n",
      "  basket: 0.5731\n",
      "  cardboard: 0.2095\n",
      "  video_tape: 0.1388\n",
      "  filament: 0.0352\n",
      "  bag: 0.3444\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 26/26 [00:53<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Train Loss: 0.4942, Train Mean IoU: 0.3729\n",
      "  film: 0.5804\n",
      "  basket: 0.5635\n",
      "  cardboard: 0.4877\n",
      "  video_tape: 0.1228\n",
      "  filament: 0.0512\n",
      "  bag: 0.4318\n",
      "Epoch [13/50], Val Mean IoU: 0.3078\n",
      "  film: 0.4572\n",
      "  basket: 0.5392\n",
      "  cardboard: 0.3281\n",
      "  video_tape: 0.1011\n",
      "  filament: 0.0513\n",
      "  bag: 0.3700\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 26/26 [01:09<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Train Loss: 0.4936, Train Mean IoU: 0.3826\n",
      "  film: 0.6651\n",
      "  basket: 0.3499\n",
      "  cardboard: 0.4275\n",
      "  video_tape: 0.1979\n",
      "  filament: 0.0265\n",
      "  bag: 0.6286\n",
      "Epoch [14/50], Val Mean IoU: 0.2957\n",
      "  film: 0.4704\n",
      "  basket: 0.3519\n",
      "  cardboard: 0.2922\n",
      "  video_tape: 0.1589\n",
      "  filament: 0.0315\n",
      "  bag: 0.4694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 26/26 [01:20<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Train Loss: 0.4829, Train Mean IoU: 0.4289\n",
      "  film: 0.7423\n",
      "  basket: 0.5689\n",
      "  cardboard: 0.4841\n",
      "  video_tape: 0.1368\n",
      "  filament: 0.0618\n",
      "  bag: 0.5793\n",
      "Epoch [15/50], Val Mean IoU: 0.3276\n",
      "  film: 0.5102\n",
      "  basket: 0.5616\n",
      "  cardboard: 0.3001\n",
      "  video_tape: 0.1110\n",
      "  filament: 0.0424\n",
      "  bag: 0.4402\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 26/26 [01:04<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Train Loss: 0.4768, Train Mean IoU: 0.4282\n",
      "  film: 0.7627\n",
      "  basket: 0.6029\n",
      "  cardboard: 0.3008\n",
      "  video_tape: 0.2412\n",
      "  filament: 0.1125\n",
      "  bag: 0.5491\n",
      "Epoch [16/50], Val Mean IoU: 0.3367\n",
      "  film: 0.5271\n",
      "  basket: 0.5795\n",
      "  cardboard: 0.2247\n",
      "  video_tape: 0.1737\n",
      "  filament: 0.1073\n",
      "  bag: 0.4077\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 26/26 [00:55<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/50], Train Loss: 0.4622, Train Mean IoU: 0.4411\n",
      "  film: 0.5885\n",
      "  basket: 0.7675\n",
      "  cardboard: 0.4855\n",
      "  video_tape: 0.1425\n",
      "  filament: 0.0947\n",
      "  bag: 0.5678\n",
      "Epoch [17/50], Val Mean IoU: 0.3386\n",
      "  film: 0.4418\n",
      "  basket: 0.6489\n",
      "  cardboard: 0.3097\n",
      "  video_tape: 0.1138\n",
      "  filament: 0.0776\n",
      "  bag: 0.4399\n",
      "Best model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50:  69%|██████▉   | 18/26 [00:24<00:11,  1.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 336\u001b[0m\n\u001b[0;32m    334\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    335\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 336\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rgb, hyper, masks \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    337\u001b[0m     rgb \u001b[38;5;241m=\u001b[39m rgb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    338\u001b[0m     hyper \u001b[38;5;241m=\u001b[39m hyper\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[1;32mIn[1], line 78\u001b[0m, in \u001b[0;36mFusionDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Resize hyperspectral image to (256, 256)\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hyper\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hyper\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m256\u001b[39m:\n\u001b[1;32m---> 78\u001b[0m     hyper \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(hyper\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Load and process mask\u001b[39;00m\n\u001b[0;32m     81\u001b[0m mask \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(mask_path)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\hackathon\\Lib\\site-packages\\torch\\nn\\functional.py:4693\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   4684\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mare_deterministic_algorithms_enabled() \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m   4685\u001b[0m             \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mis_xpu\n\u001b[0;32m   4686\u001b[0m         ):\n\u001b[0;32m   4687\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put\u001b[39;00m\n\u001b[0;32m   4688\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[0;32m   4689\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[0;32m   4690\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\n\u001b[0;32m   4691\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4692\u001b[0m             )\u001b[38;5;241m.\u001b[39m_upsample_linear_vec(\u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[1;32m-> 4693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_bilinear2d(\n\u001b[0;32m   4694\u001b[0m         \u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors\n\u001b[0;32m   4695\u001b[0m     )\n\u001b[0;32m   4696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   4697\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, ColorJitter, RandomHorizontalFlip, RandomRotation\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tifffile\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define class names and settings\n",
    "class_names = [\"background\", \"film\", \"basket\", \"cardboard\", \"video_tape\", \"filament\", \"bag\"]\n",
    "num_classes = len(class_names)\n",
    "ignore_in_eval = [True, False, False, False, False, False, False]  # Ignore background in evaluation\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# Dataset class for RGB, hyperspectral, and mask data\n",
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, rgb_dir, hyper_dir, mask_dir, rgb_transform=None, hyper_transform=None, mask_transform=None):\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.hyper_dir = hyper_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.rgb_transform = rgb_transform\n",
    "        self.hyper_transform = hyper_transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "        self.rgb_filenames = sorted(os.listdir(rgb_dir))\n",
    "        self.hyper_filenames = sorted(os.listdir(hyper_dir))\n",
    "        self.mask_filenames = sorted(os.listdir(mask_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_path = os.path.join(self.rgb_dir, self.rgb_filenames[idx])\n",
    "        hyper_path = os.path.join(self.hyper_dir, self.hyper_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
    "\n",
    "        # Load RGB image\n",
    "        rgb = Image.open(rgb_path).convert(\"RGB\")\n",
    "        rgb = self.rgb_transform(rgb) if self.rgb_transform else ToTensor()(rgb)\n",
    "\n",
    "        # Load hyperspectral image (TIF)\n",
    "        hyper = tifffile.imread(hyper_path).astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "        hyper = torch.tensor(hyper, dtype=torch.float32)\n",
    "\n",
    "        # Ensure hyperspectral image shape is [C, H, W] with 33 channels\n",
    "        if len(hyper.shape) == 3:  # Expected [H, W, C]\n",
    "            hyper = hyper.permute(2, 0, 1)  # Convert to [C, H, W]\n",
    "        elif len(hyper.shape) == 2:  # If grayscale, add channel dimension\n",
    "            hyper = hyper.unsqueeze(0)\n",
    "\n",
    "        # Select first 33 bands if more exist\n",
    "        if hyper.shape[0] > 33:\n",
    "            hyper = hyper[:33]\n",
    "        elif hyper.shape[0] < 33:\n",
    "            raise ValueError(f\"Hyperspectral image has {hyper.shape[0]} channels, but 33 are required!\")\n",
    "\n",
    "        # Resize hyperspectral image to (256, 256)\n",
    "        if hyper.shape[1] != 256 or hyper.shape[2] != 256:\n",
    "            hyper = F.interpolate(hyper.unsqueeze(0), size=(256, 256), mode=\"bilinear\", align_corners=True).squeeze(0)\n",
    "\n",
    "        # Load and process mask\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = torch.tensor(np.array(mask), dtype=torch.long)\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return rgb, hyper, mask\n",
    "\n",
    "# Transformations for training (RGB and masks)\n",
    "train_image_transform = Compose([\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomRotation(degrees=20),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_mask_transform = Compose([\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomRotation(degrees=20),\n",
    "    Lambda(lambda x: torch.tensor(np.array(x), dtype=torch.long))\n",
    "])\n",
    "\n",
    "# Transformations for validation (RGB only)\n",
    "val_image_transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_mask_transform = Compose([])\n",
    "\n",
    "# Dataset paths (update these to your actual paths)\n",
    "rgb_train_dir = \"./rgb/train\"\n",
    "hyper_train_dir = \"./hyper/train\"\n",
    "mask_train_dir = \"./labels_rgb/train\"\n",
    "\n",
    "# Initialize dataset\n",
    "dataset = FusionDataset(\n",
    "    rgb_dir=rgb_train_dir,\n",
    "    hyper_dir=hyper_train_dir,\n",
    "    mask_dir=mask_train_dir,\n",
    "    rgb_transform=None,\n",
    "    hyper_transform=None,\n",
    "    mask_transform=None\n",
    ")\n",
    "\n",
    "# Stratified train/validation split\n",
    "labels = [1 if np.any(mask.numpy() > 0) else 0 for _, _, mask in dataset]\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(split.split(np.zeros(len(labels)), labels))\n",
    "\n",
    "# Subset datasets\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "# Apply transformations\n",
    "train_dataset.dataset.rgb_transform = train_image_transform\n",
    "train_dataset.dataset.mask_transform = train_mask_transform\n",
    "val_dataset.dataset.rgb_transform = val_image_transform\n",
    "val_dataset.dataset.mask_transform = val_mask_transform\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Compute class weights for imbalanced data\n",
    "def compute_class_weights(dataset, num_classes):\n",
    "    mask_flat = []\n",
    "    for _, _, mask in dataset:\n",
    "        mask_flat.extend(mask.numpy().flatten())\n",
    "    class_weights = compute_class_weight(\"balanced\", classes=np.arange(num_classes), y=mask_flat)\n",
    "    return torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "class_weights = compute_class_weights(train_dataset, num_classes)\n",
    "\n",
    "# Combined loss function\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs_soft = F.softmax(inputs, dim=1)\n",
    "        targets_onehot = F.one_hot(targets, num_classes=inputs.shape[1]).permute(0, 3, 1, 2).float()\n",
    "        intersection = torch.sum(inputs_soft * targets_onehot, dim=(0, 2, 3))\n",
    "        union = torch.sum(inputs_soft + targets_onehot, dim=(0, 2, 3))\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        log_probs = F.log_softmax(inputs, dim=1)\n",
    "        probs = torch.exp(log_probs)\n",
    "        loss = (1 - probs) ** self.gamma * self.ce(inputs, targets)\n",
    "        return loss.mean()\n",
    "\n",
    "criterion = lambda outputs, targets: 0.5 * FocalLoss(weight=class_weights)(outputs, targets) + 0.5 * DiceLoss()(outputs, targets)\n",
    "\n",
    "# FusionUNet model definition\n",
    "class FusionUNet(nn.Module):\n",
    "    def __init__(self, num_classes, hyperspectral_bands=33):\n",
    "        super(FusionUNet, self).__init__()\n",
    "\n",
    "        # Initial convolutional blocks for RGB and hyperspectral inputs\n",
    "        self.initial_conv_rgb = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.initial_conv_hyper = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # RGB Encoder (ResNet34 backbone)\n",
    "        self.rgb_encoder = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "        self.rgb_initial = nn.Sequential(*list(self.rgb_encoder.children())[:3])\n",
    "        self.rgb_maxpool = list(self.rgb_encoder.children())[3]\n",
    "        self.rgb_encoder1 = list(self.rgb_encoder.children())[4]\n",
    "        self.rgb_encoder2 = list(self.rgb_encoder.children())[5]\n",
    "        self.rgb_encoder3 = list(self.rgb_encoder.children())[6]\n",
    "        self.rgb_encoder4 = list(self.rgb_encoder.children())[7]\n",
    "\n",
    "        # Hyperspectral Encoder with spectral attention\n",
    "        self.spectral_attention = nn.Sequential(\n",
    "            nn.Conv2d(hyperspectral_bands, 16, kernel_size=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=1)\n",
    "        )\n",
    "        self.hyper_encoder = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "        self.hyper_initial = nn.Sequential(*list(self.hyper_encoder.children())[:3])\n",
    "        self.hyper_maxpool = list(self.hyper_encoder.children())[3]\n",
    "        self.hyper_encoder1 = list(self.hyper_encoder.children())[4]\n",
    "        self.hyper_encoder2 = list(self.hyper_encoder.children())[5]\n",
    "        self.hyper_encoder3 = list(self.hyper_encoder.children())[6]\n",
    "        self.hyper_encoder4 = list(self.hyper_encoder.children())[7]\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self._decoder_block(1024, 512)\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self._decoder_block(512, 256)\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self._decoder_block(256, 128)\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self._decoder_block(64, 64)\n",
    "        self.upconv0 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.decoder0 = self._decoder_block(96, 32)  # 96 = 32 (from upconv0) + 64 (from initial_features)\n",
    "        self.conv_last = nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "\n",
    "    def _decoder_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, rgb, hyper):\n",
    "        # Hyperspectral processing\n",
    "        hyper = self.spectral_attention(hyper)\n",
    "\n",
    "        # Initial feature extraction for skip connection\n",
    "        initial_features_rgb = self.initial_conv_rgb(rgb)        # [batch, 32, 256, 256]\n",
    "        initial_features_hyper = self.initial_conv_hyper(hyper)  # [batch, 32, 256, 256]\n",
    "        initial_features = torch.cat((initial_features_rgb, initial_features_hyper), dim=1)  # [batch, 64, 256, 256]\n",
    "\n",
    "        # RGB Encoder\n",
    "        rgb0 = self.rgb_initial(rgb)\n",
    "        rgb1 = self.rgb_maxpool(rgb0)\n",
    "        rgb1 = self.rgb_encoder1(rgb1)\n",
    "        rgb2 = self.rgb_encoder2(rgb1)\n",
    "        rgb3 = self.rgb_encoder3(rgb2)\n",
    "        rgb4 = self.rgb_encoder4(rgb3)\n",
    "\n",
    "        # Hyperspectral Encoder\n",
    "        hyper0 = self.hyper_initial(hyper)\n",
    "        hyper1 = self.hyper_maxpool(hyper0)\n",
    "        hyper1 = self.hyper_encoder1(hyper1)\n",
    "        hyper2 = self.hyper_encoder2(hyper1)\n",
    "        hyper3 = self.hyper_encoder3(hyper2)\n",
    "        hyper4 = self.hyper_encoder4(hyper3)\n",
    "\n",
    "        # Concatenate encoder outputs\n",
    "        enc4 = torch.cat((rgb4, hyper4), dim=1)\n",
    "        enc3 = torch.cat((rgb3, hyper3), dim=1)\n",
    "        enc2 = torch.cat((rgb2, hyper2), dim=1)\n",
    "        enc1 = torch.cat((rgb1, hyper1), dim=1)\n",
    "\n",
    "        # Decoder\n",
    "        d4 = self.upconv4(enc4)\n",
    "        d4 = torch.cat((d4, enc3), dim=1)\n",
    "        d4 = self.decoder4(d4)\n",
    "\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = torch.cat((d3, enc2), dim=1)\n",
    "        d3 = self.decoder3(d3)\n",
    "\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat((d2, enc1), dim=1)\n",
    "        d2 = self.decoder2(d2)\n",
    "\n",
    "        d1 = self.upconv1(d2)\n",
    "        d1 = self.decoder1(d1)\n",
    "\n",
    "        # Additional upsampling with skip connection\n",
    "        d0 = self.upconv0(d1)                        # [batch, 32, 256, 256]\n",
    "        d0 = torch.cat((d0, initial_features), dim=1)  # [batch, 32 + 64 = 96, 256, 256]\n",
    "        d0 = self.decoder0(d0)                       # [batch, 32, 256, 256]\n",
    "        return self.conv_last(d0)                    # [batch, num_classes, 256, 256]\n",
    "\n",
    "# Initialize model, optimizer, and scheduler\n",
    "model = FusionUNet(num_classes, hyperspectral_bands=33).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, min_lr=1e-6)\n",
    "\n",
    "# IoU calculation function\n",
    "def calculate_iou(preds, masks, num_classes):\n",
    "    intersection = torch.zeros(num_classes).to(preds.device)\n",
    "    union = torch.zeros(num_classes).to(preds.device)\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        if ignore_in_eval[cls]:\n",
    "            continue\n",
    "        pred_inds = (preds == cls)\n",
    "        target_inds = (masks == cls)\n",
    "        intersection[cls] += (pred_inds & target_inds).sum().float()\n",
    "        union[cls] += (pred_inds | target_inds).sum().float()\n",
    "    \n",
    "    iou_per_class = [(intersection[cls] / union[cls]).item() if union[cls].item() > 0 else float('nan') for cls in range(num_classes)]\n",
    "    valid_ious = [iou for iou in iou_per_class if not np.isnan(iou)]\n",
    "    mean_iou = np.mean(valid_ious) if valid_ious else 0.0\n",
    "    \n",
    "    return mean_iou, iou_per_class\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "best_iou = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for rgb, hyper, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        rgb = rgb.to(device)\n",
    "        hyper = hyper.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(rgb, hyper)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Evaluate on Training Set\n",
    "    model.eval()\n",
    "    train_preds, train_masks = [], []\n",
    "    with torch.no_grad():\n",
    "        for rgb, hyper, masks in train_loader:\n",
    "            rgb = rgb.to(device)\n",
    "            hyper = hyper.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(rgb, hyper)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_preds.append(preds.cpu())\n",
    "            train_masks.append(masks.cpu())\n",
    "    \n",
    "    train_preds = torch.cat(train_preds, dim=0)\n",
    "    train_masks = torch.cat(train_masks, dim=0)\n",
    "    train_mean_iou, train_iou_per_class = calculate_iou(train_preds, train_masks, num_classes)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss / len(train_loader):.4f}, Train Mean IoU: {train_mean_iou:.4f}\")\n",
    "    for cls, iou in enumerate(train_iou_per_class):\n",
    "        if not ignore_in_eval[cls]:\n",
    "            print(f\"  {class_names[cls]}: {iou:.4f}\")\n",
    "    \n",
    "    # Evaluate on Validation Set\n",
    "    val_preds, val_masks = [], []\n",
    "    with torch.no_grad():\n",
    "        for rgb, hyper, masks in val_loader:\n",
    "            rgb = rgb.to(device)\n",
    "            hyper = hyper.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(rgb, hyper)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_preds.append(preds.cpu())\n",
    "            val_masks.append(masks.cpu())\n",
    "    \n",
    "    val_preds = torch.cat(val_preds, dim=0)\n",
    "    val_masks = torch.cat(val_masks, dim=0)\n",
    "    val_mean_iou, val_iou_per_class = calculate_iou(val_preds, val_masks, num_classes)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Mean IoU: {val_mean_iou:.4f}\")\n",
    "    for cls, iou in enumerate(val_iou_per_class):\n",
    "        if not ignore_in_eval[cls]:\n",
    "            print(f\"  {class_names[cls]}: {iou:.4f}\")\n",
    "    \n",
    "    # Save Best Model\n",
    "    if val_mean_iou > best_iou:\n",
    "        best_iou = val_mean_iou\n",
    "        torch.save(model.state_dict(), \"best_fusion_model.pth\")\n",
    "        print(\"Best model saved!\")\n",
    "    \n",
    "    scheduler.step(val_mean_iou)\n",
    "\n",
    "# Final Evaluation Function\n",
    "def evaluate_model(model, loader, dataset_name):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_masks = [], []\n",
    "    with torch.no_grad():\n",
    "        for rgb, hyper, masks in loader:\n",
    "            rgb = rgb.to(device)\n",
    "            hyper = hyper.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(rgb, hyper)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item() * rgb.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_masks.append(masks.cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_masks = torch.cat(all_masks, dim=0)\n",
    "    mean_iou, iou_per_class = calculate_iou(all_preds, all_masks, num_classes)\n",
    "    \n",
    "    total_loss /= len(loader.dataset)\n",
    "    print(f\"{dataset_name} Results:\")\n",
    "    print(f\"Loss: {total_loss:.4f}\")\n",
    "    print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "    for cls, iou in enumerate(iou_per_class):\n",
    "        if not ignore_in_eval[cls]:\n",
    "            print(f\"  {class_names[cls]}: {iou:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Load and evaluate best model\n",
    "model.load_state_dict(torch.load(\"best_fusion_model.pth\"))\n",
    "print(\"Final Model Evaluation:\")\n",
    "evaluate_model(model, train_loader, \"Training Set\")\n",
    "evaluate_model(model, val_loader, \"Validation Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283aefd9-6780-487a-82a7-605cae76b83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
